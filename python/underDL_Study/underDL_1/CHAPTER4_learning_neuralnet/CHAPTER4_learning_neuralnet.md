# 밑바닥부터 시작하는 딥러닝 1권

## CHAPTER 4:  신경망 학습

### 4.1 데이터에서 학습한다!

신경망의 특징은 데이터를 보고 학습할 수 있다는 점이다.

학습: 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것.



#### 4.1.1 데이터 주도학습

데이터가 이끄는 접근 방식 덕에 사람 중심 접근에서 벗어날 수 있었다. 사람이라면 어렵지 않게 인식할 수 있는 문제들이지만, 그 안에 숨은 규칙성을 명확한 로직으로 풀기가 쉽지 않고 시간도 오래 걸린다.

주어진 이미지에서 특징(feature. 입력 데이터에서 변환된 중요한 데이터)을 추출하는 방법이 있다. (컴퓨터 비전 분야: SIFT, SURF, HOG 등) 이런 특징을 사용하여 이미지 데이터를 벡터로 변환하고, 변환된 벡터로 지도학습으로 학습할 수 있다.

신경망은 입력 이미지를 있는 그대로 학습하며, 이미지에 포함된 중요한 특징까지도 기계가 스스로 학습한다.

- 사람이 생각한 알고리즘 ㅡ> 결과
- 사람이 생각한 특징(SIFT, HOG, 등)을 통한 기계학습 ㅡ> 결과
- 신경망(딥러닝) ㅡ> 결과



#### 4.1.2 훈련 데이터와 시험 데이터

훈련 데이터 (training data) : 학습용. 최적 매개변수 탐색용

시험 데이터 (test data) : 학습된(훈련된) 모델의 평가용

- 범용 능력: 아직 보지 못한 데이터에도 좋은 성능을 보이는 것

오버피팅(overfitting, 과적합) : 한 데이터셋에만 최적화된 상태. 범용 능력이 떨어진다.



### 4.2 손실 함수

신경망에서 지표(ex. 손실 함수(비용 함수) )를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색해야한다.

일반적으로 오차제곱합과 교차 엔트로피 오차를 사용한다.



#### 4.2.1 오차제곱합

 ![image-20210206225629000](CHAPTER4_learning_neuralnet.assets/image-20210206225629000.png)

k : 데이터의 차원 수

yk : 신경망의 출력(예측값)

tk : 정답



one-hot encoding : 정답 후보 레이블들의 값을 펼쳐서 인덱스로 한 리스트로 만든다. 정답인 원소만 1로 하고 그 외는 0으로 나타내는 표기법

ex: 

정답 레이블 : 딥러닝

정답 후보 레이블들: 밑바닥, 시작, 딥러닝

one-hot encoding을 거친 정답 레이블: 0 0 1



오차제곱합 코드:

```python
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
def sum_squares_error(y, t):
    return 0.5 * np.sum((y-t)**2)

# 2로 예측
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
sum_squares_error(np.array(y), np.array(t))
# 결과: 0.09750000

# 7로 예측
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
sum_squares_error(np.array(y), np.array(t))
# 결과: 0.59750000
```

손실 함수가 작은 예측이 정답 레이블과의 오차도 작다.





#### 4.2.2 교차 엔트로피 오차

 ![image-20210206231239420](CHAPTER4_learning_neuralnet.assets/image-20210206231239420.png)

log : 밑이 e인 자연로그(log_e)이다.

y_k : 신경망의 출력

t_k : 정답 레이블 (one-hot encoding)



정답이 아닌 나머지 모두는 t_k가 0이므로 log y_k와 곱해도 0이 되어 결과에 영향을 주지 않는다. 그래서 결과적으로 정답일 때의 추정(tk 가 1일 때의 yk)의 자연로그를 계산하는 식이 된다.

교차 엔트로피 오차는 정답일 때의 출력(확률)이 손실 함수의 전체 값을 정하게 된다.



