# CHAPTER4 : WORD2VEC 속도 개선

CBOW 모델은 단순 2층 신경망이라서 간단하게 구현할 수 있었으나 말뭉치에 포함된 어휘 수가 많아지면 계산량도 커진다는 문제가 있었다.

word2vec의 속도 개선을 알아보자.

첫 번째 개선 : `Embedding 계층`을 도입한다.

두 번째 개선 : `네거티브 샘플링` 손실 함수를 도입한다.





## 4.1 word2vec 개선_1

앞 장의 CBOW 모델은 단어 2개를 맥락으로 사용해, 하나의 단어(타깃)를 추측한다.

![image-20210316233429850](CHAPTER4_WORD2VEC_IMPROVE.assets/image-20210316233429850.png)

입력 측 가중치(W_in)와의 행렬 곱으로 은닉층이 계산되고, 다시 출력 측 가중치(W_out)와의 행렬 곱으로 각 단어의 점수를 구한다. 이 점수에 소프트맥스 함수를 적용해 각 단어의 출현 확률을 얻고, 이 확률을 정답 레이블과 비교(교차 엔트로피 오차를 적용)하여 손실을 구한다.

하지만 어휘가 많을 때는 뉴런의 갯수가 많아져 중간 계산에 많은 시간이 소요되어서 병목 현상이 일어난다. 문제가 되는 계산은 다음과 같다.

- 입력 층의 원핫 표현과 가중치 행렬 W_in의 곱 계산
  - 입력층의 원핫 표현과 관련한 문제이다. 어휘 수가 많아지면 원핫 표현의 벡터 크기도 커지는 것이다. 상당한 메모리를 차지한다. 
- 은닉층과 가중치 행렬 W_out의 곱 및 Softmax 계층의 계산
  - 은닉층 이후의 계산에 관련된 문제이다. 은닉층과 가중치 행렬 W_out의 곱만 해도 계산량이 크다. Softmax 계층에서도 다루는 어휘가 많아짐에 따라 계산량이 증가한다.



### 4.1.1 Embedding 계층

가중치 매개변수로부터 '단어 ID에 해당하는 행(벡터)'을 추출하는 계층(`Embedding 계층`)을 만들어보자. Embedding 계층에 단어 임베딩(분산 표현)을 저장하는 것이다.



### 4.1.2 Embedding 계층 구현

행렬에서 특정 행을 추출하기란 아주 쉽다. 가중치 W가 2차원 넘파이 배열일 때, 이 가중치로부터 특정 행을 추출하려면 그저 W[2]나 W[5]처럼 원하는 행을 명시하면 끝이다.

```python
import numpy as np
W = np.arange(21).reshape(7, 3)
W

W[2] # [6, 7, 8]
W[5] # [15, 16, 17]
```

가중치 W로부터 여러 행을 추출하는 일도 간단하다. 원하는 행번호들을 배열에 명시하면 된다.

```python
idx = np.array([1, 0, 3, 0]) # 인덱스 4개 추출
W[idx]
# array([[3, 4, 5],[0, 1, 2], [9, 10, 11], [0, 1, 2]])
```

인수에 배열을 사용하면 여러 행을 한꺼번에 출력 가능하다.



Embedding 계층의 forward() 메서드 구현

```python
class Embedding:
    def __init__(self, W):
        self.params = [W]
        self.grads = [np.zeros_like(W)]
        self.idex = None
        
    def forward(self, idx):
        W, = self.params
        self.idx = idx
        out = W[idx]
        return out
```

인스턴스 변수 params와 grads를 사용한다. 또한 인스턴스 변수 idx에는 추출하는 행의 인덱스(단어 ID)를 배열로 저장한다.



Embedding 계층의 순전파는 가중치 W의 특정 행을 추출할 뿐이다. 따라서 역전파(backward)에서는 앞 층(출력 측 층)으로부터 전해진 기울기를 다음 층(입력 측 층)으로 그대로 흘려주면 된다. 다만, 앞 층으로부터 전해진 기울기를 가중치 기울기 dW의 특정 행(idx번째 행)에 설정한다.

















