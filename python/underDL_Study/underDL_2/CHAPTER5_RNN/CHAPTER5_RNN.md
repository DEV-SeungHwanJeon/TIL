# CHAPTER5 : 순환 신경망(RNN)

단순 피드포워드 신경망에서는 시계열 데이터의 성질(패턴)을 충분히 학습할 수 없다. 그래서 순환 신경망(Recurrent Neural Network)이 등장한다.



## 5.1 확률과 언어 모델



### 5.1.1  word2vec을 확률 관점에서 바라보다

CBOW 모델은 맥락으로부터 타깃을 추측하는 일을 수행한다.

 ![image-20210324233221989](CHAPTER5_RNN.assets/image-20210324233221989.png)

맥락이 주어졌을 때 타깃이 w_t가 될 확률 수식: ![image-20210324233256677](CHAPTER5_RNN.assets/image-20210324233256677.png)

CBOW 모델은 위 수식의 사후 확률을 모델링 한다. 이 사후 확률은 "`w_(t-1)`과 `w_(t+1)`이 주어졌을 때 w_t가 일어날 확률"을 뜻한다.



이번에는 맥락을 왼쪽 윈도우만으로 한정해보자.

 ![image-20210324233442448](CHAPTER5_RNN.assets/image-20210324233442448.png)

수식은 이렇게 바뀐다 : ![image-20210324233506038](CHAPTER5_RNN.assets/image-20210324233506038.png)

손실 함수 : ![image-20210324233618007](CHAPTER5_RNN.assets/image-20210324233618007.png)

CBOW 모델의 학습은 말뭉치 전체의 손실함수의 총합을 최소화 하는 가중치 매개변수를 찾는 것이다. 이러한 가중치 매개변수가 발견되면 CBOW 모델은 맥락으로부터 타깃을 더 정확하게 추측할 수 있게 된다. 단어의 분산 표현은 부산물이다.



### 5.1.2 언어 모델

언어 모델은 단어 나열에 확률을 부여한다. 특정한 단어의 시퀀스에 대해서, 그 시퀀스가 일어날 가능성이 어느 정도인지를 확률로 평가한다.

음성 인식 시스템의 경우, 사람의 음성으로부터 몇 개의 문장을 후보로 생성하고 언어 모델을 사용하여 후보 문장이 '문장으로써 자연스러운지'를 기준으로 순서를 매긴다.

새로운 문장을 생성하는 용도로도 이용할 수 있다. 언어 모델은 단어 순서의 자연스러움을 확률적으로 평가할 수 있으므로, 그 확률분포에 따라 다음으로 적합한 단어를 샘플링할 수 있다.



언어 모델을 수식으로 설명:

- w_1 ~ w_m 으로 이루어진 문장(m개의 단어).

- P(w_1, ..., w_m) : 단어가 w_1 ~ w_m 순서로 출현할 확률 ( 이 확률은 여러 사건이 동시에 일어날 확률이므로 동시 확률이라고 함 )

- 이 동시 확률은 사후 확률의 총곱으로 나타낼 수 있다.

   ![image-20210324234717231](CHAPTER5_RNN.assets/image-20210324234717231.png)

- 이 결과는 확률의 곱셈정리에서 유도된다. ( m개 단어의 동시 확률을 사후 확률로 나타낼 수 있다. )
  -  ![image-20210324235022904](CHAPTER5_RNN.assets/image-20210324235022904.png)
  - A와 B가 모두 일어날 확률 P(A, B)는 B가 일어날 확률 P(B)와 B가 일어난 후 A가 일어날 확률 P(A|B)를 곱한 값과 같다.
  - 반대 순서로도 가능하다.
- 식 변형 순서
  -  ![image-20210324235215400](CHAPTER5_RNN.assets/image-20210324235215400.png)
    - w_1 ~ w_(m-1) 을 하나로 모아 기호 A로 나타낸다.
  -  ![image-20210324235322005](CHAPTER5_RNN.assets/image-20210324235322005.png)
  - 이처럼 단어 시퀀스를 하나씩 줄여가며 매번 사후 확률로 분해해간다. 이 과정을 반복하여 
  -  ![image-20210324234717231](CHAPTER5_RNN.assets/image-20210324234717231.png)
  - 가 나온다. 
  - 주목할 점은 이 사후 확률은 타깃단어보다 왼쪽에 있는 모든 단어를 맥락(조건)으로 했을 떄의 확률이라는 것이다.
  -  ![image-20210324235618564](CHAPTER5_RNN.assets/image-20210324235618564.png)



정리하면, 우리의 목표는 ![image-20210324235654195](CHAPTER5_RNN.assets/image-20210324235654195.png) 이며 이 확률을 계산할 수 있다면 언어 모델의 동시 확률 ![image-20210324235715987](CHAPTER5_RNN.assets/image-20210324235715987.png)을 구할 수 있다.





### 5.1.3 CBOW 모델을 언어 모델로?

word2vec의 CBOW 모델을 언어 모델에 적용하려면 맥락의 크기를 특정 값으로 한정하여 근사적으로 나타낼 수 있다.

 ![image-20210325002831490](CHAPTER5_RNN.assets/image-20210325002831490.png)

CBOW 모델의 사후 확률에 따라 근사적으로 나타낼 수 있다. 맥락의 크기는 임의로 설정할 수 있어도 결국 특정 길이로 고정된다. 그 맥락보다 더 왼쪽에 있는 단어의 정보는 무시된다. ![image-20210325004446821](CHAPTER5_RNN.assets/image-20210325004446821.png)

이 문제에서 정답을 구하려면 예문의 "?"로부터 18번째나 앞에 나오는 "Tom"을 기억해야 한다. 만약 CBOW 모델의 맥락이 10개까지였다면 이 문제에 제대로 답할 수 없다. 또한 CBOW 모델에서는 맥락 안의 단어 순서가 무시된다는 한계가 있다. ( 은닉층에서 단어 벡터들이 더해지므로 맥락의 단어 순서는 무시된다 )

![image-20210325011112272](CHAPTER5_RNN.assets/image-20210325011112272.png)

오른쪽 처럼 맥락의 단어 벡터를 은닉층에서 연결하는 방식을 생각할 수도 있다. 그러나 연결하는 방식을 취하면 맥락의 크기에 비례해 가중치 매개변수도 늘어난다.

RNN은 맥락이 아무리 길더라도 그 맥락의 정보를 기억하는 메커니즘을 갖추고 있다.



## 5.2 RNN이란

## 5.2.1 순환하는 신경망

'닫힌 경로' 혹은 '순환하는 경로'가 존재해야 데이터가 같은 장로를 반복해 왕래(순환)할 수 있다. 데이터가 순환하면서 정보가 끊임없이 갱신되게 된다.

RNN 계층은 순환하는 경로를 포함한다. 

 ![image-20210326231804185](CHAPTER5_RNN.assets/image-20210326231804185.png)

x_t 는 입력이고 t는 시각이다. 각 시각에 입력되는 x_t는 벡터라고 가정한다. 문장(단어 순서)을 다루는 경우를 예로 든다면 각 단어의 분산 표현(단어 벡터)이 x_t가 되며, 이 분산 표현이 순서대로 하나씩 RNN계층에 입력되는 것이다.



### 5.2.2 순환 구조 펼치기

분기된 출력 중 하나가 자기 자신에 입력된다.

![image-20210326234546167](CHAPTER5_RNN.assets/image-20210326234546167.png)

각 시각의 RNN 계층은 그 계층으로의 입력과 1개 전의 RNN 계층으로부터의 출력을 받는다. 이 두 정보를 바탕으로 현 시각의 출력을 계산한다. 이 때 수행하는 계산의 수식은 다음과 같다

 ![image-20210326234801784](CHAPTER5_RNN.assets/image-20210326234801784.png)

RNN에는 가중치가 2개 있다. x를 h로 변환하기 위한 가중치 w_x이고, 다른 하나는 1개의 RNN 출력을 다음 시각의 출력으로 변환하기 위한 가중치 w_h이다. 편향 b도 있다. ( h_(t-1)과 x_t 는 행 벡터이다. ) 계산 후 그 합을 tanh 함수를 이용해 변환한 결과가 시각 t의 출력 h_t가 된다.

현재의 출력 h_t는 한 시각 이전의 출력 h_(t-1)에 기초해 계산된다. 

다른 관점으로 보면 RNN은 h라는 '상태'를 가지고 있으며, 위의 식의 형태로 갱신된다. 그래서 RNN계층을 '상태를 가지는 계층' 혹은 '메모리(기억력)가 있는 계층'이라고 한다.









