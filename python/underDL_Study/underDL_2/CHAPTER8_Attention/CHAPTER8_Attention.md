# CHAPTER8_어텐션



## 8.1 어텐션의 구조



### 8.1.1 seq2seq의 문제점

seq2seq에서는 Encoder가 시계열 데이터를 인코딩하고, 그 인코딩된 정보를 Decoder로 전달한다. 이 때 Encoder의 출력은 '고정 길이 벡터'였다. 

하지만 '고정 길이'벡터는 입력 문장의 길이에 관계없이(아무리 길어도), 항상 같은 길이의 벡터로 변환한다는 뜻이다. 즉, 아무리 긴 문장이 입력되더라도 항상 똑같은 길이의 벡터에 밀어넣어야 한다. 그래서 이 '고정 길이 벡터'라는 점이 문제가 된다.

 ![image-20210415234512302](CHAPTER8_Attention.assets/image-20210415234512302.png)



### 8.1.2 Encoder 개선

Encoder 출력의 길이를 입력 문장의 길이에 따라 바꿔주자.

![image-20210415234641257](CHAPTER8_Attention.assets/image-20210415234641257.png)

각 시각(각 단어)의 은닉 상태 벡터를 모두 이용하면 입력된 단어와 같은 수의 벡터를 얻을 수 있다. 이것으로 Encoder는 '하나의 고정 길이 벡터'라는 제약을 해제한다.



여기서 주목할 점으로는 LSTM 계층의 은닉 상태의 '내용'이다. 시각별 LSTM 계층의 은닉 상태에는 직전에 입력된 단어에 대한 정보가 많이 포함되어있다. 그래서 Encoder가 출력하는 hs 행렬은 각 단어에 해당하는 벡터들의 집합이라고 볼 수 있다.

![image-20210415234904553](CHAPTER8_Attention.assets/image-20210415234904553.png)

단지 Encoder의 은닉 상태를 모든 시각만큼 꺼냈을 뿐이지만 이 작은 개선 덕분에 Encoder는 입력 문장의 길이에 비례한 정보를 인코딩할 수 있게 된다.



### 8.1.3 Decoder 개선 1

Encoder는 각 단어에 대응하는 LSTM 계층의 은닉 상태 벡터를 hs로 모아 출력한다. 이 hs가 Decoder에 전달되어 시게열 변환이 이뤄진다.

![image-20210416225422318](CHAPTER8_Attention.assets/image-20210416225422318.png)

앞 장에서는 Encoder가 hs에서 마지막 줄만 빼내어 Decoder에 전달하였다.

이번에는 이 hs 전부를 활용할 수 있도록 Decoder를 개선해보자.



인간은 '어떤 단어(혹은 단어의 집합)'에 주목하여 그 단어의 변환을 수시로 한다. 그렇다면 seq2seq에서는 '입력과 출력의 여러 단어 중 어떤 단어끼리 서로 관련되어 있는가' 라는 대응 관계를 seq2seq에게 학습시켜보자.

목표는 '도착어 단어'와 대응 관계에 있는 '출발어 단어'의 정보를 골라내는 것. 그리고 그 정보를 이용하여 번역을 수행하는 것이다. 

즉, 필요한 정보에만 주목하여 그 정보로부터 시게열 변환을 수행하는 것이 목표이다.

이 구조를 어텐션이라고 부른다.



어텐션의 전체 틀

![image-20210416225801887](CHAPTER8_Attention.assets/image-20210416225801887.png)

새롭에 '어떤 계산'을 수행하는 계층을 추가한다. 현재까지는 Encoder의 마지막 은닉 상태 벡터는 Decoder의 첫 번째 LSTM 계층에 전달한다.

- '어떤 계산'의 입력:
  - Encoder로부터 받는 hs
  - 시각별 LSTM 계층의 은닉 상태
- '어떤 계산'이 하는 일:
  - 필요한 정보만 골라 위쪽의 Affine 계층으로 출력



그러나 신경망으로 하고 싶은 일은 단어들의 얼라인먼트 추출이다. 각 시각에서 Decoder에 입력된 단어와 대응 관계인 단어의 벡터를 hs에서 골라내겠다는 뜻이다.

- 얼라인먼트 : 단어(혹은 문구)의 대응 관계를 나타내는 정보. 지금까지는 주로 사람이 수작업으로 만들었다. 어텐션 기술은 얼라인먼트라는 아이디어를 seq2seq에 자동으로 도입하는데 성공했다.

예를 들면 Decoder가 "I"를 출력할 때, hs에서 "나"에 대응하는 벡터를 선택하면 된다. 그리고 이러한 '선택' 작업을 '어떤 계산'으로 하려한다. 하지만 선택하는 작업 (여러 대상으로부터 몇 개를 선택하는 작업)은 미분할 수 없다는 문제가 생긴다.

'하나를 선택'하는 게 아니라, '모든 것을 선택'한다는 아이디어로 '선택한다'라는 작업을 미분 가능한 연산으로 대체할 수 있다. 또한 각 단어의 중요도(기여도)를 나타내는 '가중치'를 별도로 계산하도록 한다.

![image-20210416230314955](CHAPTER8_Attention.assets/image-20210416230314955.png)

위 그림에서 보듯, 여기에서는 각 단어의 중요도를 나타내는 '가중치'(기호 a)를 이용한다. a는 확률분포처럼 각 원소가 0~1 사이의 스칼라이며, 모든 원소의 총합은 1이 된다. 각 단어의 중요도를 나타내는 가중치 a와 각 단어의 벡터 hs로부터 가중치합을 구하여 우리가 원하는 벡터를 얻는다.

![image-20210416230410651](CHAPTER8_Attention.assets/image-20210416230410651.png)

단어 벡터의 가중합을 계산한다. 이 결과를 '맥락 벡터'(기호 c)라고 부른다.

'나'에 대응하는 가중치가 0.8이다. 이것은 맥락 벡터 c에는 '나' 벡터의 성분이 많이 포함되어있다는 것을 의미한다. 즉, '나' 벡터를 '선택'하는 작업을 이 가중합으로 대체하고 있다.



Encoder가 출력하는 hs와 각 단어의 가중치a를 적당하게 작성하고, 그 가중합을 구하는 구현

```python
import numpy as np
T, H = 5, 4
hs = np.random.rand(T, H)
a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])

ar = a.reshape(5,1).repeat(4, axis=1)
print(ar.shape)

t = hs * ar
print(t.shape)

c = np.sum(t, axis=0)
print(c.shape)
```

시계열의 길이 T=5, 은닉 상태 벡터의 원소 수 H=4 로 하여 가중합을 구하는 과정을 보여준다. 

ar = a.reshape(5,1).repeat(4, axis=1) 코드 설명:

![image-20210416230726169](CHAPTER8_Attention.assets/image-20210416230726169.png)

이 코드는 ar = hs * a.reshape(5,1) 와 같다. (넘파이의 브로드캐스트 )

