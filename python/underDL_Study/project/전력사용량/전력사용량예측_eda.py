# -*- coding: utf-8 -*-
"""전력사용량예측_EDA

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dw2MhgauuLkF220Oexl6baaJC0ygtkJY

# README

코드 진짜 중구난방 많이 더럽습니다...

바로 밑에 있는 # 1짜리 **전력 사용량 예측** 접으시면 바로 밑에

'cluster 이후입니다. 여기서부터 돌리시면 됩니다!' 가 뜹니다.

그 셀 안에 있는게 그나마 좀 정상..적이며 제가 제출한 결과 코드 입니다!
"""

!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf
!pip install PyWavelets

"""# 전력사용량예측

- 데이터 분석 및 전처리

- 건물 클러스터링

- 슬라이딩 윈도우

- 디노이징 필터

- 모델링 (LSTM)

### 데이터 import 및 결측치 확인
"""

import numpy as np
import pandas as pd
import math
import os
import matplotlib.pyplot as plt
plt.rc('font', family='NanumBarunGothic') 

import sweetviz as sv
import pywt

train_col = ['building_num', 'date_time', 'target', 'temperatures', 'wind_speed', 'humidity', 'precipitation', 'sunshine', 'non_electric_cooling_facility', 'solarpower']
test_col = ['building_num', 'date_time', 'temperatures', 'wind_speed', 'humidity', 'precipitation_6hours', 'sunshine_3hours', 'non_electric_cooling_facility', 'solarpower']

train_df=pd.read_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/train.csv', encoding='CP949')
train_df.columns = train_col

test_df=pd.read_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/test.csv', encoding='CP949')
test_df.columns = test_col

submission=pd.read_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/sample_submission.csv', encoding='CP949')

train_df.isna().sum()

train_df.date_time = train_df.date_time.astype('datetime64')
train_df.set_index('date_time', inplace=True)

train_df

building_num=1
temp_df = train_df[train_df['building_num']==building_num].copy()
col_num = 0
row_num = 0
fig, ax = plt.subplots(4, 2, constrained_layout=True, figsize=(25,10))

for col in ['target', 'temperatures', 'wind_speed', 'humidity', 'precipitation', 'sunshine', 'non_electric_cooling_facility', 'solarpower']:
  ax[row_num, col_num].plot(temp_df.index, temp_df[col], 'b')
  ax[row_num, col_num].set_title(str(building_num)+'_'+col)

  ax2 = ax[row_num, col_num].twinx()
  ax2.plot(temp_df.index, temp_df['target'], 'r-')
  # ax[row_num, col_num].set_ylim(7800,8800, 0, 1)

  
  col_num+=1
  if col_num == 2:
    col_num = 0
    row_num += 1
  
  # plt.plot(temp_df.index, temp_df[col] ,temp_df.index, temp_df['target'], 'r-') 
fig.tight_layout()
plt.show()

building_num+=1
temp_df = train_df[train_df['building_num']==building_num].copy()
col_num = 0
row_num = 0
fig, ax = plt.subplots(4, 2, constrained_layout=True, figsize=(25,10))

for col in ['target', 'temperatures', 'wind_speed', 'humidity', 'precipitation', 'sunshine', 'non_electric_cooling_facility', 'solarpower']:
  ax[row_num, col_num].plot(temp_df.index, temp_df[col], 'b')
  ax[row_num, col_num].set_title(str(building_num)+'_'+col)

  ax2 = ax[row_num, col_num].twinx()
  ax2.plot(temp_df.index, temp_df['target'], 'r-')
  # ax[row_num, col_num].set_ylim(7800,8800, 0, 1)

  
  col_num+=1
  if col_num == 2:
    col_num = 0
    row_num += 1
  
  # plt.plot(temp_df.index, temp_df[col] ,temp_df.index, temp_df['target'], 'r-') 
fig.tight_layout()
plt.show()

"""### 시계열 분해"""

import statsmodels.api as sm
from scipy import stats

building_num = 2
temp_df = train_df[train_df['building_num']==building_num].copy()
plt.rcParams['figure.figsize'] = (25,15)
sm.tsa.seasonal_decompose(temp_df['target'], model='additive').plot()
plt.show()

"""### 웨이블릿 변환을 통한 디노이징"""

def maddest(d, axis=None):
    return np.mean(np.absolute(d - np.mean(d, axis)), axis)

def denoise_signal(x, wavelet='db4', level=1):
    coeff = pywt.wavedec(x, wavelet, mode="per")     # pywt.wavedec : https://pywavelets.readthedocs.io/en/latest/ref/dwt-discrete-wavelet-transform.html?highlight=wavedec#pywt.wavedec
    sigma = (1/0.6745) * maddest(coeff[-level])

    uthresh = sigma * np.sqrt(2*np.log(len(x)))
    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])  # pywt.threshold : https://pywavelets.readthedocs.io/en/latest/ref/thresholding-functions.html

    return pywt.waverec(coeff, wavelet, mode='per')

building_num=1
temp_df = train_df[train_df['building_num']==building_num].copy()
col_num = 0
row_num = 0
fig, ax = plt.subplots(4, 2, constrained_layout=True, figsize=(25,10))

for col in ['target', 'temperatures', 'wind_speed', 'humidity', 'precipitation', 'sunshine', 'non_electric_cooling_facility', 'solarpower']:
  ax[row_num, col_num].plot(temp_df.index, temp_df[col], 'b')
  ax[row_num, col_num].set_title(str(building_num)+'_'+col)

  ax2 = ax[row_num, col_num].twinx()
  ax2.plot(temp_df.index, denoise_signal(temp_df[col]), 'r-')
  # ax[row_num, col_num].set_ylim(7800,8800, 0, 1)

  
  col_num+=1
  if col_num == 2:
    col_num = 0
    row_num += 1
  
  # plt.plot(temp_df.index, temp_df[col] ,temp_df.index, temp_df['target'], 'r-') 
fig.tight_layout()
plt.show()

"""##### 웨이블릿 변환 결과 크게 보기 ( temperatures, wind_speed, humidity )"""

building_num=1
temp_df = train_df[train_df['building_num']==building_num].copy()
col_num = 0
row_num = 0
fig, ax = plt.subplots(1, 1, constrained_layout=True, figsize=(25,8))

for col in ['temperatures']:
  ax.plot(temp_df.index, temp_df[col], 'b')
  ax.set_title(str(building_num)+'_'+col)

  ax2 = ax.twinx()
  ax2.plot(temp_df.index, denoise_signal(temp_df[col]), 'r-')

building_num=1
temp_df = train_df[train_df['building_num']==building_num].copy()
col_num = 0
row_num = 0
fig, ax = plt.subplots(1, 1, constrained_layout=True, figsize=(25,8))

for col in ['wind_speed']:
  ax.plot(temp_df.index, temp_df[col], 'b')
  ax.set_title(str(building_num)+'_'+col)

  ax2 = ax.twinx()
  ax2.plot(temp_df.index, denoise_signal(temp_df[col]), 'r-')

building_num=1
temp_df = train_df[train_df['building_num']==building_num].copy()
col_num = 0
row_num = 0
fig, ax = plt.subplots(1, 1, constrained_layout=True, figsize=(25,8))

for col in ['humidity']:
  ax.plot(temp_df.index, temp_df[col], 'b')
  ax.set_title(str(building_num)+'_'+col)

  ax2 = ax.twinx()
  ax2.plot(temp_df.index, denoise_signal(temp_df[col]), 'r-')





test_df.isna().sum()

feature_config = sv.FeatureConfig(force_cat=["building_num", "non_electric_cooling_facility", "solarpower"])
analyze_report = sv.analyze(source = train_df, target_feat='target', feat_cfg = feature_config)

analyze_report.show_notebook(w=1400, h=700) # scale=0.8

train_df

"""#### 슬라이딩 윈도우
- 빌딩번호 1~60
- 몇일전까지 하는게 좋을지는 추후 코드 학습해보면서 판단하기
"""

def makeSlidingWindows(origin_train, train, lookBack=3, multiVariate=False):  # origin_train, origin_test, train, test, lookBack=3, multiVariate=False
    # if not multiVariate:
    #     train_df.columns = origin_train.columns
    #     test_df.columns = origin_test.columns
    #     column_list = list(train_df)
        
    #     for s in range(1, lookBack + 1):
    #         tmp_train = train_df[column_list].shift(s)
    #         tmp_test = test_df[column_list].shift(s)

    #         tmp_train.columns = "shift_" + tmp_train.columns + "_" + str(s)
    #         tmp_test.columns = "shift_" + tmp_test.columns + "_" + str(s)

    #         train_df[tmp_train.columns] = train_df[column_list].shift(s)
    #         test_df[tmp_test.columns] = test_df[column_list].shift(s)

    #     return train_df, test_df

    # else:
    train_df = pd.DataFrame(train, index=origin_train.index)
    # test_df = pd.DataFrame(test, index=origin_test.index)
    train_df.columns = origin_train.columns
    # test_df.columns = origin_test.columns
    column_list = list(train_df)

    for s in range(1, lookBack + 1):
        tmp_train = train_df[column_list].shift(s)
        # tmp_test = test_df[column_list].shift(s)

        tmp_train.columns = "shift_" + tmp_train.columns + "_" + str(s)
        # tmp_test.columns = "shift_" + tmp_test.columns + "_" + str(s)

        train_df[tmp_train.columns] = train_df[column_list].shift(s)
        # test_df[tmp_test.columns] = test_df[column_list].shift(s)

    return train_df

# 1~60 building을 돌면서 슬라이딩 윈도우 실시
total_result_df = pd.DataFrame()
for i in range(1, 61):
  building_num = i
  temp_df = train_df[train_df['building_num']==building_num].copy()
  temp_df_for_windows = temp_df[['temperatures',	'wind_speed',	'humidity',	'precipitation',	'sunshine']].copy()
  result_df_for_windows = temp_df_for_windows.copy()
  # 3일전꺼까지 포함
  result_df_for_windows = makeSlidingWindows(temp_df_for_windows, result_df_for_windows, lookBack=3, multiVariate=True)
  # 슬라이딩 윈도우 대상이 아닌 columns와 슬라이딩윈도우 결과 합치기
  result_df = pd.concat([temp_df[['building_num', 'target', 'non_electric_cooling_facility', 'solarpower']], result_df_for_windows], axis=1)
  # 최종 df에 합치기
  total_result_df= pd.concat([total_result_df, result_df], axis=0)

test.fillna(method="ffill").fillna(0)

total_result_df.dropna()

"""## 데이콘 예시"""

import numpy as np
import pandas as pd
import math
import os
import matplotlib.pyplot as plt
plt.rc('font', family='NanumBarunGothic') 
from sklearn.metrics import mean_absolute_error

#######딥러닝 라이브러리##########
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Reshape, GRU, RNN

tf.keras.backend.set_floatx('float64')

train_col = ['building_num', 'date_time', 'target', 'temperatures', 'wind_speed', 'humidity', 'precipitation', 'sunshine', 'non_electric_cooling_facility', 'solarpower']
test_col = ['building_num', 'date_time', 'temperatures', 'wind_speed', 'humidity', 'precipitation_6hours', 'sunshine_3hours', 'non_electric_cooling_facility', 'solarpower']

train=pd.read_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/train.csv', encoding='CP949')
train.columns = train_col

test=pd.read_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/test.csv', encoding='CP949')
test.columns = test_col

submission=pd.read_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/sample_submission.csv', encoding='CP949')

##############전력사용량(kWh) 정규화####################################
mini=train.iloc[:,2].min()
size=train.iloc[:,2].max()-train.iloc[:,2].min()
train.iloc[:,2]=(train.iloc[:,2]-mini)/size

input_window = 840 #임의의 수
output_window = 24 #168 7일 24시간
window = 12 #window는 12시간 마다
num_features = 6 #베이스라인은 feature를 하나만 사용했습니다.
num_power = 60
end_=168
lstm_units=32
dropout=0.2
EPOCH=50
BATCH_SIZE=128

train[['target','temperatures', 'wind_speed', 'humidity', 'precipitation', 'sunshine']]

#train을 tensor로 변경 (60, 24*85, 1)
train_x=tf.reshape(train[['target','temperatures', 'wind_speed', 'humidity', 'precipitation', 'sunshine']].values, [num_power, 24*85, num_features])
print(f'train_x.shape:{train_x.shape}')

#train_window_x np.zeros를 만듬 (60, 85, 996, 1)
train_window_x= np.zeros(( train_x.shape[0], (train_x.shape[1]-(input_window + output_window))//window, input_window, num_features)) 
train_window_y= np.zeros(( train_x.shape[0], (train_x.shape[1]-(input_window + output_window))//window, output_window, 1))
print(f'train_window_x.shape:{train_window_x.shape}')
print(f'train_window_y.shape:{train_window_y.shape}')

#train_window_x에 train값 채워넣기
for example in range(train_x.shape[0]):
    
    for start in range(0, train_x.shape[1]-(input_window+output_window), window):
        end=start+input_window
        train_window_x[example, start//window, :] = train_x[example, start: end               , :]
        train_window_y[example, start//window, :] = train_x[example, end  : end+ output_window, 1]

train_x

#new_train_x, reshape통해 lstm에 알맞은 형태로 집어넣기
new_train_x=tf.reshape(train_window_x, [-1, input_window, num_features])
new_train_y=tf.reshape(train_window_y, [-1, output_window,num_features])
print(f'new_train_x.shape:{new_train_x.shape}')
print(f'new_train_y.shape:{new_train_y.shape}')

#####층 쌓기###########

model=Sequential([
  LSTM(lstm_units, return_sequences=False, recurrent_dropout=dropout),
  Dense(output_window * num_features, kernel_initializer=tf.initializers.zeros()), 
  Reshape([output_window, num_features])
])

#######Compile 구성하기################

model.compile(optimizer='rmsprop', loss='mae', metrics=['mae'])
# 에포크가 끝날 때마다 점(.)을 출력해 훈련 진행 과정을 표시합니다
class PrintDot(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs):
        if epoch % 10 == 0: print('')
        print('.', end='')

#가장 좋은 성능을 낸 (val_loss가 적은) model만 남겨 놓았습니다.
save_best_only=tf.keras.callbacks.ModelCheckpoint(filepath="lstm_model.h5", monitor='val_loss', save_best_only=True)

early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)

#검증 손실이 10epoch 동안 좋아지지 않으면 학습률을 0.1 배로 재구성하는 명령어입니다.
reduceLR = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10)

######################
model.fit(new_train_x, new_train_y, epochs=EPOCH, batch_size=BATCH_SIZE, validation_split = 0.2, verbose=0,
          callbacks=[PrintDot(), early_stop, save_best_only , reduceLR])

model.summary()

#######################
prediction=np.zeros((num_power, end_, num_features))
new_test_x=train_x

for i in range(end_//output_window):
    start_=i*output_window
    next_=model.predict(new_test_x[ : , -input_window:, :])
    new_test_x = tf.concat([new_test_x, next_], axis=1)
    print(new_test_x.shape)
    prediction[:, start_: start_ + output_window, :]= next_
prediction =prediction *size + mini

prediction

submission['answer']=prediction.reshape([-1,1])
submission

submission.to_csv('baseline_submission1.csv', index=False)

"""해야할 일
- 모델 피팅 시 validation을 마음대로 할 수 있는지
- building num을 one-hot encoding시키기
- LSTM 층 늘리는 방법 공부 (input_dim, output_dim 에 대해서)
"""



"""## Pytorch로 해보기"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

import torch
import torch.nn as nn
import torch.optim as optim

import matplotlib.pyplot as plt

"""### 이전 전처리"""

train_col = ['building_num', 'date_time', 'target', 'temperatures', 'wind_speed', 'humidity', 'precipitation', 'sunshine', 'non_electric_cooling_facility', 'solarpower']
test_col = ['building_num', 'date_time', 'temperatures', 'wind_speed', 'humidity', 'precipitation', 'sunshine', 'non_electric_cooling_facility', 'solarpower']

train_df=pd.read_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/train.csv', encoding='CP949')
train_df['date_time'] = pd.to_datetime(train_df['date_time'])
train_df.columns = train_col

test_df=pd.read_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/test.csv', encoding='CP949')
test_df.columns = test_col
test_df['date_time'] = pd.to_datetime(test_df['date_time'])

sample_submission=pd.read_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/sample_submission.csv', encoding='CP949')

train_col = ['building_num', 'date_time', 'target', 'temperatures', 'wind_speed', 'humidity', 'precipitation', 'sunshine', 'non_electric_cooling_facility', 'solarpower']
test_col = ['building_num', 'date_time', 'temperatures', 'wind_speed', 'humidity', 'precipitation', 'sunshine', 'non_electric_cooling_facility', 'solarpower']

train_df=pd.read_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/train.csv', encoding='CP949')
train_df.columns = train_col
train_df['date_time'] = pd.to_datetime(train_df['date_time'])
# train_df = pd.read_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/train_transform.csv')
# train_df = train_df.drop(columns=['Unnamed: 0'])


test_df=pd.read_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/test.csv', encoding='CP949')
test_df.columns = test_col
test_df['date_time'] = pd.to_datetime(test_df['date_time'])

submission=pd.read_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/sample_submission.csv', encoding='CP949')

train_df.drop_duplicates(subset = ['building_num', 'non_electric_cooling_facility', 'solarpower']).dropna()

sun = train_df.groupby('building_num')['solarpower'].unique()
non_ele = train_df.groupby('building_num')['non_electric_cooling_facility'].unique()

test_df['solarpower'] = test_df['building_num'].map(sun).astype(int)
test_df['non_electric_cooling_facility'] = test_df['building_num'].map(non_ele).astype(int)

test_df = test_df.interpolate()

train_df['weekday'] = train_df['date_time'].dt.weekday + 1
train_df['hour'] = train_df['date_time'].dt.hour

test_df['weekday'] = test_df['date_time'].dt.weekday + 1
test_df['hour'] = test_df['date_time'].dt.hour

train_df.head(3)

real_train_df = train_df.copy()

# 세아아부지
cluster1 = [4, 10, 11, 12, 34, 40, 41, 42]
cluster2 = [35,6,48,27,57,8,25,56,26,55,47,13,53,18,7, 17, 46]
cluster3 = [31,33,9,3,1,32]
cluster4 = [29,38,43,58,15,22,39,54,23,44,45,37,52,2,14]
cluster5 = [21,19,50,49,20,51,30,36,28,59,5,60,16,24]
total_cluster = [cluster1,cluster2,cluster3,cluster4,cluster5]
train_df['cluster'] = 0
for idx in train_df.index:
  building_num = train_df.loc[idx,['building_num']].values[0]
  # print(building_num.values[0], end='')
  if building_num in cluster1:
    train_df.loc[idx,['cluster']] = 1
  elif building_num in cluster2:
    train_df.loc[idx,['cluster']] = 2
  elif building_num in cluster3:
    train_df.loc[idx,['cluster']] = 3
  elif building_num in cluster4:
    train_df.loc[idx,['cluster']] = 4
  else:
    train_df.loc[idx,['cluster']] = 5

real_test_df = test_df.copy()

for idx in test_df.index:
  building_num = test_df.loc[idx,['building_num']].values[0]
  # print(building_num.values[0], end='')
  if building_num in cluster1:
    test_df.loc[idx,['cluster']] = 1
  elif building_num in cluster2:
    test_df.loc[idx,['cluster']] = 2
  elif building_num in cluster3:
    test_df.loc[idx,['cluster']] = 3
  elif building_num in cluster4:
    test_df.loc[idx,['cluster']] = 4
  else:
    test_df.loc[idx,['cluster']] = 5

# real_train_df.to_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/train_transform.csv')
# train_df.to_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/train_transform_cluster.csv')

# real_test_df.to_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/test_transform.csv')
# test_df.to_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/test_transform_cluster.csv')

"""# cluster 이후입니다. 여기서부터 돌리시면 됩니다!"""

# real_train_df = pd.read_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/train_transform.csv')
# real_train_df = real_train_df.drop(columns=['Unnamed: 0'])
# real_test_df = pd.read_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/test_transform.csv')
# real_test_df = real_test_df.drop(columns=['Unnamed: 0'])

train_df = pd.read_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/train_transform_cluster.csv')
train_df = train_df.drop(columns=['Unnamed: 0'])
train_df['date_time'] = pd.to_datetime(train_df['date_time'])


test_df = pd.read_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/test_transform_cluster.csv')
test_df = test_df.drop(columns=['Unnamed: 0'])
test_df['date_time'] = pd.to_datetime(test_df['date_time'])

train_df.set_index('date_time', inplace=True)
test_df.set_index('date_time', inplace=True)
test_df['cluster'] = test_df['cluster'].astype(int)

# seasonal 에서 trend를 이용할 순 없을까? -> 모델에 적합해서 예측하고 test_df 에 넣어볼까?
# train_seasonal = sm.tsa.seasonal_decompose(train_df[train_df['building_num']==building_num]['target'], model='additive')
# from sklearn.linear_model import LinearRegression
# model = LinearRegression()
# train_seasonal.trend.dropna()

import statsmodels.api as sm
from scipy import stats
train_df['seasonal']=0
test_df['seasonal']=0

temp_train_seasonal_list = []
temp_test_seasonal_list = []
for building_num in range(1, 61):
  temp_train_df = train_df[train_df['building_num']==building_num].copy()
  temp_train_seasonal = sm.tsa.seasonal_decompose(temp_train_df['target'], model='additive')
  temp_train_seasonal_list += temp_train_seasonal.seasonal.to_list()
  temp_test_seasonal_list += temp_train_seasonal.seasonal.to_list()[:168]

train_df['seasonal'] = temp_train_seasonal_list
test_df['seasonal'] = temp_test_seasonal_list

train_df = pd.get_dummies(train_df, columns=['cluster'])
test_df = pd.get_dummies(test_df, columns=['cluster'])
train_df.index = list(range(train_df.shape[0]))
test_df.index = list(range(test_df.shape[0]))

def SMAPE(true, pred):
    return np.mean((np.abs(true-pred))/(np.abs(true) + np.abs(pred)))*100

X_trains = [train_df[train_df['building_num']==building_num].drop(columns=['target','building_num']).copy() for building_num in range(1,61)]
y_trains = [train_df[train_df['building_num']==building_num][['target']].copy() for building_num in range(1,61)]
X_tests = [test_df[test_df['building_num']==building_num].drop(columns=['building_num']).copy() for building_num in range(1, 61)]

print(X_trains[0].columns)
print(X_tests[0].columns)

from lightgbm import LGBMRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler


models_list = []
total_pred = []
for i in range(0, 60):
  cross=KFold(n_splits=5, shuffle=False, random_state=42)
  folds=[]
  train_x = X_trains[i]
  train_y = y_trains[i]
  test_x = X_tests[i]

  feature_scaler = StandardScaler()
  train_x[['temperatures', 'wind_speed','humidity', 'precipitation', 'sunshine', 'seasonal']] = feature_scaler.fit_transform(train_x[['temperatures', 'wind_speed','humidity', 'precipitation', 'sunshine', 'seasonal']])
  test_x[['temperatures', 'wind_speed','humidity', 'precipitation', 'sunshine', 'seasonal']] = feature_scaler.transform(test_x[['temperatures', 'wind_speed','humidity', 'precipitation', 'sunshine', 'seasonal']])

  target_scaler = StandardScaler()
  train_y[['target']] = target_scaler.fit_transform(train_y[['target']].values)

  for train_idx, valid_idx in cross.split(train_x, train_y):
    folds.append((train_idx, valid_idx))

  models={}
  for fold in range(5):
    print(f'===================={fold+1}=======================')
    train_idx, valid_idx=folds[fold]
    X_train=train_x.iloc[train_idx, :]
    y_train=train_y.iloc[train_idx, :]
    X_valid=train_x.iloc[valid_idx, :]
    y_valid=train_y.iloc[valid_idx, :]
    
    # model=RandomForestRegressor(n_estimators=100)

    model = LGBMRegressor(n_estimators=50)
    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], 
            early_stopping_rounds=30)
    models[fold]=model
    print(f'================================================\n\n')
  models_list.append(models)

  temp_df = pd.DataFrame(columns=['answer'])
  # print(models[0].predict(test_x)/5 )
  # print(type(models[0].predict(test_x)/5))
  # print(len(models[0].predict(test_x)/5))
  for i in range(5):
    if i == 0:
      answer = models[i].predict(test_x)/5 
    else:
      answer += models[i].predict(test_x)/5 
  print(answer)
  # print(type(answer))
  # print(len(answer))
  print(target_scaler.inverse_transform(answer))
  # print(type(target_scaler.inverse_transform(answer)))
  
  total_pred += target_scaler.inverse_transform(answer).tolist()
  # print(temp_df)

sample_submission=pd.read_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/sample_submission.csv', encoding='CP949')
sample_submission['answer'] =  total_pred
sample_submission.to_csv('/content/drive/MyDrive/데이터분석/밑시딥/전력예측/energy/lstm2_submission.csv', index=False)

"""# 여기서부터는... 보지마세요"""

import xgboost
models_list = []
total_pred = []
for i in range(0, 60):
  cross=KFold(n_splits=5, shuffle=False, random_state=42)
  folds=[]
  train_x = X_trains[i]
  train_y = y_trains[i]
  test_x = X_tests[i]

  feature_scaler = StandardScaler()
  train_x[['temperatures', 'wind_speed','humidity', 'precipitation', 'sunshine', 'seasonal']] = feature_scaler.fit_transform(train_x[['temperatures', 'wind_speed','humidity', 'precipitation', 'sunshine', 'seasonal']])
  test_x[['temperatures', 'wind_speed','humidity', 'precipitation', 'sunshine', 'seasonal']] = feature_scaler.transform(test_x[['temperatures', 'wind_speed','humidity', 'precipitation', 'sunshine', 'seasonal']])

  target_scaler = StandardScaler()
  train_y[['target']] = target_scaler.fit_transform(train_y[['target']].values)

  for train_idx, valid_idx in cross.split(train_x, train_y):
    folds.append((train_idx, valid_idx))

  models={}
  for fold in range(5):
    print(f'===================={fold+1}=======================')
    train_idx, valid_idx=folds[fold]
    X_train=train_x.iloc[train_idx, :]
    y_train=train_y.iloc[train_idx, :]
    X_valid=train_x.iloc[valid_idx, :]
    y_valid=train_y.iloc[valid_idx, :]
    
    # model=RandomForestRegressor(n_estimators=100)

    model = XGBRegressor(n_estimators=200, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)
    model.fit(X_train, y_train)
    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
    # evaluate model
    scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)
    # force scores to be positive
    scores = absolute(scores)
    print('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()) )
    
    models[fold]=model
    print(f'================================================\n\n')
  models_list.append(models)

  temp_df = pd.DataFrame(columns=['answer'])
  # print(models[0].predict(test_x)/5 )
  # print(type(models[0].predict(test_x)/5))
  # print(len(models[0].predict(test_x)/5))
  for i in range(5):
    if i == 0:
      answer = models[i].predict(test_x)/5 
    else:
      answer += models[i].predict(test_x)/5 
  print(answer)
  # print(type(answer))
  # print(len(answer))
  print(target_scaler.inverse_transform(answer))
  # print(type(target_scaler.inverse_transform(answer)))
  
  total_pred += target_scaler.inverse_transform(answer).tolist()
  # print(temp_df)

X = train[['temperatures', 'wind_speed', 'humidity',
        'precipitation', 'sunshine', 'non_electric_cooling_facility',
        'solarpower', 'seasonal']].values
y = train['target'].values



train_df.date_time = train_df.date_time.astype('datetime64')
train_df.set_index('date_time', inplace=True)

test_df.date_time = test_df.date_time.astype('datetime64')
test_df.set_index('date_time', inplace=True)

import statsmodels.api as sm
from scipy import stats

def smape(A, F):
    return 100 / len(A) * np.sum(np.abs(F - A) / (np.abs(A) + np.abs(F)))



X = train[['temperatures', 'wind_speed', 'humidity',
         'precipitation', 'sunshine', 'non_electric_cooling_facility',
         'solarpower', 'seasonal']].values
  y = train['target'].values

"""### 한방코드1"""

import statsmodels.api as sm
from scipy import stats

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(f'{device} is available')

def smape(A, F):
    return 100 / len(A) * np.sum(np.abs(F - A) / (np.abs(A) + np.abs(F)))

def seq_data(x, y, sequence_length):
  x_seq = []
  y_seq = []
  for i in range(len(x) - sequence_length):
    x_seq.append(x[i: i+sequence_length])
    y_seq.append(y[i+sequence_length])
  return torch.FloatTensor(x_seq).to(device), torch.FloatTensor(y_seq).to(device).view([-1, 1]) # float형 tensor로 변형, gpu사용가능하게 .to(device)를 사용.


def smape_cal(train_loader, test_loader, actual):
  with torch.no_grad():
    train_pred = []
    test_pred = []

    for data in train_loader:
      seq, target = data
      out = model(seq)
      train_pred += out.cpu().numpy().tolist()

    for data in test_loader:
      seq, target = data
      out = model(seq)
      test_pred += out.cpu().numpy().tolist()
  
  total = train_pred + test_pred
  total = target_scaler.inverse_transform(np.array(total).reshape(-1,1))
  return smape(actual, total)
  # plt.figure(figsize=(20,10))
  # plt.plot(np.ones(100)*len(train_pred), np.linspace(actual.min()-300,actual.max()+300,100), '--', linewidth=0.6)
  # plt.plot(actual, '--')
  # plt.plot(total, 'b', linewidth=0.6)

  # plt.legend(['train boundary', 'actual', 'prediction'])
  # plt.show()

class LSTM1(nn.Module):
  def __init__(self, input_size, hidden_size, sequence_length, num_layers, device):
    super(LSTM1, self).__init__()
    self.device = device
    self.hidden_size = hidden_size
    self.num_layers = num_layers
    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
    self.fc = nn.Sequential(nn.Linear(hidden_size * sequence_length, 128),
                            nn.Linear(128, 1),
                            nn.Sigmoid())
    
  def forward(self, x):
    h0 = torch.zeros(self.num_layers, x.size()[0], self.hidden_size).to(self.device) # 초기 hidden state 설정하기.
    c0 = torch.zeros(self.num_layers, x.size()[0], self.hidden_size).to(self.device) # 초기 hidden state 설정하기.
    out, _ = self.lstm(x, (h0, c0)) # out: RNN의 마지막 레이어로부터 나온 output feature 를 반환한다. hn: hidden state를 반환한다.
    out = out.reshape(out.shape[0], -1) # many to many 전략
    out = self.fc(out)
    return out



SMAPE_LIST = []
MODEL_LIST = []

for building_number in range(1, 61):
  train = train_df[train_df['building_num']==building_number]
  test = test_df[test_df['building_num']==building_number]

  if test.iloc[0, 6] != 1:
    test[['non_electric_cooling_facility']] = test[['non_electric_cooling_facility']].fillna(0)
  else:
    test[['non_electric_cooling_facility']] = test[['non_electric_cooling_facility']].fillna(1)

  if test.iloc[0, 7] != 1:
    test[['solarpower']] = test[['solarpower']].fillna(0)
  else:
    test[['solarpower']] = test[['solarpower']].fillna(1)

  test.iloc[163, 4] = test.iloc[162, 4]
  test.iloc[164, 4] = test.iloc[162, 4]
  test.iloc[165, 4] = test.iloc[162, 4]
  test.iloc[166, 4] = test.iloc[162, 4]
  test.iloc[167, 4] = test.iloc[162, 4]
  test.iloc[166, 5] = test.iloc[165, 5]
  test.iloc[167, 5] = test.iloc[165, 5]
  test = test.interpolate()
  
  train_seasonal = sm.tsa.seasonal_decompose(train['target'], model='additive')
  # train['resid'] = train_seasonal.resid.fillna(method='bfill').fillna(method='ffill')
  train['seasonal'] = train_seasonal.seasonal
  # train['trend'] = train_seasonal.trend.fillna(method='bfill').fillna(method='ffill')
  test[['seasonal']] = train.iloc[0:168, 9].values
  test[['target']] = 0
  test = test[['building_num', 'target', 'temperatures', 'wind_speed', 'humidity',
        'precipitation', 'sunshine', 'non_electric_cooling_facility',
        'solarpower', 'seasonal']]

  feature_scaler = MinMaxScaler()
  train[['temperatures', 'wind_speed','humidity', 'precipitation', 'sunshine', 'seasonal']] = feature_scaler.fit_transform(train[['temperatures', 'wind_speed','humidity', 'precipitation', 'sunshine', 'seasonal']])
  target_scaler = MinMaxScaler()
  train[['target']] = target_scaler.fit_transform(train[['target']].values)

  X = train[['temperatures', 'wind_speed', 'humidity',
         'precipitation', 'sunshine', 'non_electric_cooling_facility',
         'solarpower', 'seasonal']].values
  y = train['target'].values

  split = 1632
  sequence_length = 24

  x_seq, y_seq = seq_data(X, y, sequence_length)

  x_train_seq = x_seq[:split]
  y_train_seq = y_seq[:split]
  x_val_seq = x_seq[split:]
  y_val_seq = y_seq[split:]

  train = torch.utils.data.TensorDataset(x_train_seq, y_train_seq)
  val = torch.utils.data.TensorDataset(x_val_seq, y_val_seq)

  batch_size = 20
  train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=batch_size, shuffle=False)
  val_loader = torch.utils.data.DataLoader(dataset=val, batch_size=batch_size, shuffle=False)

  input_size = x_seq.size(2)
  num_layers = 2
  hidden_size = 8

  model = LSTM1(input_size=input_size,
                    hidden_size=hidden_size,
                    sequence_length=sequence_length,
                    num_layers=num_layers,
                    device=device).to(device)
  print(model)

  criterion = nn.MSELoss()
  lr = 1e-3
  num_epochs = 2000
  optimizer = optim.Adam(model.parameters(), lr=lr)

  loss_graph = [] # 그래프 그릴 목적인 loss.
  n = len(train_loader)
  print("ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡbuilding",building_number,'학습 시작ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ')
  for epoch in range(1,num_epochs+1):
    running_loss = 0.0

    for data in train_loader:

      seq, target = data # 배치 데이터.
      out = model(seq)   # 모델에 넣고,
      loss = criterion(out, target) # output 가지고 loss 구하고,

      optimizer.zero_grad() # 
      loss.backward() # loss가 최소가 되게하는 
      optimizer.step() # 가중치 업데이트 해주고,
      running_loss += loss.item() # 한 배치의 loss 더해주고,

    loss_graph.append(running_loss / n) # 한 epoch에 모든 배치들에 대한 평균 loss 리스트에 담고,
    if epoch == 0 or epoch % 100 == 0:
      print('[epoch: %d] loss: %.4f'%(epoch, running_loss/n))


  smape_loss = smape_cal(train_loader, val_loader, target_scaler.inverse_transform(y[sequence_length:].reshape(-1,1)))
  print('SMAPE', smape_loss)
  SMAPE_LIST.append(smape_loss)
  model_name = 'building'+str(building_num)+'_lstm.pt'
  torch.save(model, '/content/drive/MyDrive/데이터분석/밑시딥/전력예측/models/'+model_name)


  break

"""### 각각 코드"""

building_number = 2
train = train_df[train_df['building_num']==building_number]
test = test_df[test_df['building_num']==building_number]

import statsmodels.api as sm
from scipy import stats

if test.iloc[0, 6] != 1:
  test[['non_electric_cooling_facility']] = test[['non_electric_cooling_facility']].fillna(0)
else:
  test[['non_electric_cooling_facility']] = test[['non_electric_cooling_facility']].fillna(1)

if test.iloc[0, 7] != 1:
  test[['solarpower']] = test[['solarpower']].fillna(0)
else:
  test[['solarpower']] = test[['solarpower']].fillna(1)

test.iloc[163, 4] = test.iloc[162, 4]
test.iloc[164, 4] = test.iloc[162, 4]
test.iloc[165, 4] = test.iloc[162, 4]
test.iloc[166, 4] = test.iloc[162, 4]
test.iloc[167, 4] = test.iloc[162, 4]
test.iloc[166, 5] = test.iloc[165, 5]
test.iloc[167, 5] = test.iloc[165, 5]
test = test.interpolate()

train_seasonal = sm.tsa.seasonal_decompose(train['target'], model='additive')
# train['resid'] = train_seasonal.resid.fillna(method='bfill').fillna(method='ffill')
train['seasonal'] = train_seasonal.seasonal
# train['trend'] = train_seasonal.trend.fillna(method='bfill').fillna(method='ffill')
test[['seasonal']] = train.iloc[0:168, 9].values
test[['target']] = 0
test = test[['building_num', 'target', 'temperatures', 'wind_speed', 'humidity',
       'precipitation', 'sunshine', 'non_electric_cooling_facility',
       'solarpower', 'seasonal']]

feature_scaler = MinMaxScaler()
train[['temperatures', 'wind_speed','humidity', 'precipitation', 'sunshine', 'seasonal']] = feature_scaler.fit_transform(train[['temperatures', 'wind_speed','humidity', 'precipitation', 'sunshine', 'seasonal']])
target_scaler = MinMaxScaler()
train[['target']] = target_scaler.fit_transform(train[['target']].values)

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(f'{device} is available')

def seq_data(x, y, sequence_length):
  
  x_seq = []
  y_seq = []
  for i in range(len(x) - sequence_length):
    x_seq.append(x[i: i+sequence_length])
    y_seq.append(y[i+sequence_length])

  return torch.FloatTensor(x_seq).to(device), torch.FloatTensor(y_seq).to(device).view([-1, 1]) # float형 tensor로 변형, gpu사용가능하게 .to(device)를 사용.

X = train[['temperatures', 'wind_speed', 'humidity',
        'precipitation', 'sunshine', 'non_electric_cooling_facility',
        'solarpower', 'seasonal']].values
y = train['target'].values

split = 1632
sequence_length = 24

x_seq, y_seq = seq_data(X, y, sequence_length)

x_train_seq = x_seq[:split]
y_train_seq = y_seq[:split]
x_val_seq = x_seq[split:]
y_val_seq = y_seq[split:]
print(x_train_seq.size(), y_train_seq.size())
print(x_val_seq.size(), y_val_seq.size())

train = torch.utils.data.TensorDataset(x_train_seq, y_train_seq)
val = torch.utils.data.TensorDataset(x_val_seq, y_val_seq)

batch_size = 20
train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=batch_size, shuffle=False)
val_loader = torch.utils.data.DataLoader(dataset=val, batch_size=batch_size, shuffle=False)

input_size = x_seq.size(2)
num_layers = 2
hidden_size = 8

"""### VanillaRNN"""

class VanillaRNN(nn.Module):

  def __init__(self, input_size, hidden_size, sequence_length, num_layers, device):
    super(VanillaRNN, self).__init__()
    self.device = device
    self.input_size = input_size
    self.hidden_size = hidden_size
    self.num_layers = num_layers
    self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
    self.fc = nn.Sequential(nn.Linear(hidden_size * sequence_length, 1), nn.Sigmoid())

  def forward(self, x):
    h0 = torch.zeros(self.num_layers, x.size()[0], self.hidden_size).to(self.device) # 초기 hidden state 설정하기.
    out, _ = self.rnn(x, h0) # out: RNN의 마지막 레이어로부터 나온 output feature 를 반환한다. hn: hidden state를 반환한다.
    out = out.reshape(out.shape[0], -1) # many to many 전략
    out = self.fc(out)
    return out

model = VanillaRNN(input_size=input_size,
                   hidden_size=hidden_size,
                   sequence_length=sequence_length,
                   num_layers=num_layers,
                   device=device).to(device)

criterion = nn.MSELoss()

lr = 1e-3
num_epochs = 1000
optimizer = optim.Adam(model.parameters(), lr=lr)

loss_graph = [] # 그래프 그릴 목적인 loss.
n = len(train_loader)

for epoch in range(1,num_epochs+1):
  running_loss = 0.0

  for data in train_loader:

    seq, target = data # 배치 데이터.
    out = model(seq)   # 모델에 넣고,
    loss = criterion(out, target) # output 가지고 loss 구하고,

    optimizer.zero_grad() # 
    loss.backward() # loss가 최소가 되게하는 
    optimizer.step() # 가중치 업데이트 해주고,
    running_loss += loss.item() # 한 배치의 loss 더해주고,

  loss_graph.append(running_loss / n) # 한 epoch에 모든 배치들에 대한 평균 loss 리스트에 담고,
  if epoch == 0 or epoch % 100 == 0:
    print('[epoch: %d] loss: %.4f'%(epoch, running_loss/n))

plt.figure(figsize=(20,10))
plt.plot(loss_graph)
plt.show()

def smape(A, F):
    return 100 / len(A) * np.sum(np.abs(F - A) / (np.abs(A) + np.abs(F)))

def plotting(train_loader, test_loader, actual):
  with torch.no_grad():
    train_pred = []
    test_pred = []

    for data in train_loader:
      seq, target = data
      out = model(seq)
      train_pred += out.cpu().numpy().tolist()

    for data in test_loader:
      seq, target = data
      out = model(seq)
      test_pred += out.cpu().numpy().tolist()
  
  total = train_pred + test_pred
  total = target_scaler.inverse_transform(np.array(total).reshape(-1,1))
  print(smape(actual, total))
  plt.figure(figsize=(20,10))
  plt.plot(np.ones(100)*len(train_pred), np.linspace(actual.min()-300,actual.max()+300,100), '--', linewidth=0.6)
  plt.plot(actual, '--')
  plt.plot(total, 'b', linewidth=0.6)

  plt.legend(['train boundary', 'actual', 'prediction'])
  plt.show()

plotting(train_loader, test_loader, target_scaler.inverse_transform(y[sequence_length:].reshape(-1,1)))



"""### LSTM"""

class LSTM1(nn.Module):

  def __init__(self, input_size, hidden_size, sequence_length, num_layers, device):
    super(LSTM1, self).__init__()
    self.device = device
    self.hidden_size = hidden_size
    self.num_layers = num_layers
    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
    self.fc = nn.Sequential(nn.Linear(hidden_size * sequence_length, 1),
                            
                             nn.Sigmoid()) #  nn.Linear(128, 1),
    
    
  def forward(self, x):
    h0 = torch.zeros(self.num_layers, x.size()[0], self.hidden_size).to(self.device) # 초기 hidden state 설정하기.
    c0 = torch.zeros(self.num_layers, x.size()[0], self.hidden_size).to(self.device) # 초기 hidden state 설정하기.
    out, _ = self.lstm(x, (h0, c0)) # out: RNN의 마지막 레이어로부터 나온 output feature 를 반환한다. hn: hidden state를 반환한다.
    out = out.reshape(out.shape[0], -1) # many to many 전략
    out = self.fc(out)
    return out
    
model = LSTM1(input_size=input_size,
                   hidden_size=hidden_size,
                   sequence_length=sequence_length,
                   num_layers=num_layers,
                   device=device).to(device)

print(model)

def smape(A, F):
    return 100 / len(A) * np.sum(np.abs(F - A) / (np.abs(A) + np.abs(F)))

def mse_loss(input, target):
  return ((input - target) ** 2).sum() / input.data.nelement()

def customized_loss(X, y):
    X_similarity = similarity_matrix(X)
    association = convert_y(y)
    loss_num = torch.sum(torch.mul(X_similarity, association))
    loss_all = torch.sum(X_similarity)
    loss_denum = loss_all - loss_num
    loss = loss_num/loss_denum
    return loss


# criterion = nn.MSELoss()
# import torch
# import torch.nn as nn
# import torch.optim as optim
lr = 1e-3
num_epochs = 1000
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')

loss_graph = [] # 그래프 그릴 목적인 loss.

def smape_torch(A, F):
    return 100 / len(A) * torch.sum(torch.abs(F - A) / (torch.abs(A) + torch.abs(F)))

n = len(train_loader)

for epoch in range(1,num_epochs+1):
  running_loss = 0.0

  for data in train_loader:

    seq, target = data # 배치 데이터.
    out = model(seq)   # 모델에 넣고,
    # loss = criterion(out, target) # output 가지고 loss 구하고,

    loss = smape_torch(out, target) # output 가지고 loss 구하고,
    
    optimizer.zero_grad() # 
    loss.backward() # loss가 최소가 되게하는 
    optimizer.step() # 가중치 업데이트 해주고,
    running_loss += loss.item() # 한 배치의 loss 더해주고,

  loss_graph.append(running_loss / n) # 한 epoch에 모든 배치들에 대한 평균 loss 리스트에 담고,
  if epoch % 100 == 0:
    scheduler.step(loss)
    print('[epoch: %d] loss: %.4f'%(epoch, running_loss/n))

def smape_cal(train_loader, test_loader, actual):
  with torch.no_grad():
    train_pred = []
    test_pred = []

    for data in train_loader:
      seq, target = data
      out = model(seq)
      train_pred += out.cpu().numpy().tolist()

    for data in test_loader:
      seq, target = data
      out = model(seq)
      test_pred += out.cpu().numpy().tolist()
  
  total = train_pred + test_pred
  total = target_scaler.inverse_transform(np.array(total).reshape(-1,1))
  return smape(actual, total), total

smape_loss, total_pred = smape_cal(train_loader, val_loader, target_scaler.inverse_transform(y[sequence_length:].reshape(-1,1)))
print('SMAPE', smape_loss)
SMAPE_LIST.append(smape_loss)
model_name = 'building'+str(building_num)+'_lstm.pt'
# torch.save(model, '/content/drive/MyDrive/데이터분석/밑시딥/전력예측/models/'+model_name)

print(total_pred)

plt.figure(figsize=(20,10))
plt.plot(loss_graph)
plt.show()

def plotting(train_loader, test_loader, actual):
  with torch.no_grad():
    train_pred = []
    test_pred = []

    for data in train_loader:
      seq, target = data
      out = model(seq)
      train_pred += out.cpu().numpy().tolist()

    for data in test_loader:
      seq, target = data
      out = model(seq)
      test_pred += out.cpu().numpy().tolist()
  
  total = train_pred + test_pred
  total = target_scaler.inverse_transform(np.array(total).reshape(-1,1))
  print(smape(actual, total))
  plt.figure(figsize=(20,10))
  plt.plot(np.ones(100)*len(train_pred), np.linspace(actual.min()-300,actual.max()+300,100), '--', linewidth=0.6)
  plt.plot(actual, '--')
  plt.plot(total, 'b', linewidth=0.6)

  plt.legend(['train boundary', 'actual', 'prediction'])
  plt.show()

plotting(train_loader, val_loader, target_scaler.inverse_transform(y[sequence_length:].reshape(-1,1)))

def smape_cal(train_loader, test_loader, actual):
  with torch.no_grad():
    train_pred = []
    test_pred = []

    for data in train_loader:
      seq, target = data
      out = model(seq)
      train_pred += out.cpu().numpy().tolist()

    for data in test_loader:
      seq, target = data
      out = model(seq)
      test_pred += out.cpu().numpy().tolist()
  
  total = train_pred + test_pred
  total = target_scaler.inverse_transform(np.array(total).reshape(-1,1))
  return smape(actual, total), total

smape_loss, total_pred = smape_cal(train_loader, val_loader, target_scaler.inverse_transform(y[sequence_length:].reshape(-1,1)))

def prediction(val_loader, test):
  with torch.no_grad():
    pred = []

    for data in val_loader:
      seq, target = data
      out = model(seq)
      pred += out.cpu().numpy().tolist()
    
  return pred

prediction(val_loader, test)

total_pred

model

x_seq.size(2)

x_seq.shape

model.eval()

for i in range(fut_pred):
    seq = torch.FloatTensor(test_inputs[-train_window:])
    with torch.no_grad():
        model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),
                        torch.zeros(1, 1, model.hidden_layer_size))
        test_inputs.append(model(seq).item())



test_inputs = train_data_normalized[-train_window:].tolist()
print(test_inputs)

test_inputs[fut_pred:]